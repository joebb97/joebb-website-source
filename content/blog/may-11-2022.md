---
title: "Languages. Mainly those used for programming."
date: 2022-05-11
---
This is all w.r.t my agreement with [this article](http://www.leancrew.com/all-this/2011/12/more-shell-less-egg/), 

and my frustration with Programming Languages that have been taken out of their design domain. 

## Some poetry about assembly
```
miss me with some donald knuth
    saying that I should do assembly language.
    miss me with some       assembly language.

miss me with some

    I = {x86, CISC},
    "CISC" == Complex Instruction Set Computer

That. I = {x86, CISC}. Am. is not for humans.
{amd64, x86}               is not for humans.

  Yes. amd64 == x86_64. Fun fact.
  In general, amd64 == x86. History!

ARM (Advanced RISC Machine) is almost, but not quite, for humans,

  "RISC" == Reduced Instruction Set Computer.
  Yes. That's what ARM stands for. Fun Fact.
```

---

## My HAWT Takes

I am a human. And I avoid reading assembly - even though I can - when I can because. Well. 
I am not a *literally* spineless, brainless piece of silicon.
I function outside the constraints of a digitally oscillating nanofabricated clock.

I am not an analog clock. One breathing with actual electricity. That clock. Does not think.
It { adds, divides, subtracts, multiplies } at the speed of GHz.
Billions of times per second (*literally*).

  Do not. Try to equate my brain with that poopy-doodoo robot brain called
{ siri, alexa, google voice assistant }. That is, total nonsense! Donald Knuth.
Here's what should probably happen.

```
START:
DO:
    while(bad code || sad || unproductive) {
SIMPLE: write POSIX shell, KISS, use simple plaintext
        if (SIMPLE doesn't work) {
          write in pseudocode
          if (sufficient) {
            continue
          } // end if
          // otherwise
          write in a real ding-danging PL // A good one! [1]

  };
END:


/* [1] A real academic, robust, "enterprise ready" programming language.
* There are many to choose from. But they are *not*. **NOT**.
* ABSOLUTELY NOT { 
    python{
      2, 2.7. 3.3, 3.7. 3.9, /etc.
    }
    (notReallyJava)Script (this one should really be JavaShit),
    TypeScript,
    Ruby
 }
* I say these languages aren't "enterprise ready" because.
* They were not designed. Adequately.
* For the problems we try to apply them to, such as {
    banking,
    ecommerce,
    social media,
    military,
    embedded devices (
        like Belkin's worthless 'digital light switches!
        it should just be a light switch. not a computer.
        change my mind. you probably won't
    ), // end embedded devices
    Smart TVs (
        seriously most of these things are giant pieces of garbage.
        Because well. 
        I'm gonna guess there's some really unperformant
        interpreted [i.e slower, that is a fact, not an opinion],
            {dynamic, duck, weakly}-typed langs in there.
    ) // end SMART TVs
* } // end "for the problems"
} // end ABSOLUTELY NOT
* 
* INSTEAD. I say the languages we should lean towards are:
*    - Compiled.
*    - Strongly Typed.
*    - (Sometimes, but not required) Statically Typed.
*    - Safe by default. Footguns are opt-in, not opt-out.
*    - (Sometimes, depending on the domain) Light on syntax.
*    - Mostly performant out of the box.
*       > You normally get this by just having a fucking compiler
          and the decades of research that comes with one.
          Blessed be thy LLVM and WASM in the future.
*    - Well researched. Well designed.
        > they have means of reducing boilerplate. like generics.
          error propagation, etc.
*    - Can do any I/O. Lol. Simon Peyton-Jones said in a talk that Haskell
       used to struggle with this at one point. No longer!
       It has I/O, concurrency, async, and it's all pretty great!
*
* Some that I've come across that fit this bill:
*    - Haskell
*    - Elm
*    - OCaml
*    - Elixir 
*    - Rust
*    - Scala (?)
*    - Julia (?)
* (?) implies that I'm just going out on a limb since I don't know for sure
*/
```

That list I just rattled off is by no means exhaustive. But `golang` is **very intentionally** left off of the list.
They just want me to write
```golang
if err != nil {
    return "someFootGunValueYouCouldUse", nil
}
```
every other line, which means it violates the points, _well researched and well designed_, and _safe by default_.

Nor am I trying to say that we shouldn't use the interpreted languages from the long list above. I'd just like to not see javascript running my bank's frontend. Is all I'm saying.

The field of PL research (whose papers I absolutely have not read btw) has come a loooong way since those grey beards concocted a kernel worth two shits at Bell Labs. Called UNIX. With the forward slashes and what not. 

Most of the research has yielded a loooot of niceties. Such as enums that are really enums (sum types I believe they're called) and not just a silly increasing number. Lookin' at you **Golang and C++**. Rust and Haskell have beautiful enums that can each carry their own data, that way you don't accidentally use the wrong data.

And of course they have composition / structures / product types (I think is what they're called). But python sure as shit didn't until 3.7 when it added `datatype`.

Just. Ask more of your languages. Especially if you are going to be using them. Every. Single. Day of the week and for most projects at your {research lab, computer company, shitty web service}. Stop defensive programming with lines of code like

```
if (thingy != nil && typeof thingy == typeIWantThingyToBe) {
    // Oh thank fucking god now I can actually use thingy without
    // worrying about this Stinger missle imploding.
    // Since it could be there's some proprietary garbage
    // language without type safety running in a Stinger missle
    // for all we know
}
```

or re-enacting the following scenario

1. Write some code like
```python
def a_function_without_types(thingy):
    pass # on this language as a whole
```
2. Then. 200 years later. You find your code not working because something has
magically changed types while your program was running because, well, your ish is duck-typed.
And now you have to, for about 5000+ lines of code, add in mind-numbing annotations like
```python
def a_function_without_types_but_now_has_them(thingy: thingyType):
    # but the type checking doesn't actually matter until you add
    # something to your project to make sure it's being done.
    # because fuck me right?
    # So it's still totally possible for you to submit programs
    # to the "compiler" MyPy and just ignore its errors, which once
    # again brings you back to unsafe by default.

    pass # on this language as a whole
```
3. Wish you'd picked a language with a compiler because that's what CS
research has pointed us to probably since fucking day one.

## Complaints I already know about

* It's too verbose!
    - Not if it's got type inference ðŸ‘ðŸ¼. Yay type inference and patterns like
    ```rust
    let x = "dingdong";
    // do things with x
    let x = Ditch();
    // do more things with x
    ```
    And if that's too verbose for ya. Check out Haskell. It's like Python but better in every way, including having farrrr fewer parentheses than most languages ever have.
* Inheritance sucks!
    - Yes it does! None of the languages I recommended above make you practice that
    trash pattern from the late 90s. Some don't even support it whatsoever!
    - Composition/structures are me and [Jon Gjengsent](https://rust-for-rustaceans.com/)'s spirit animals.
    Dynamic dispatch is nice and also achievable without. Shitty. Inheritance that looks something like
      - `Handler -> WebHandler -> ApiHandler -> ApiV2Handler -> ThingThatAlmostDoesHTTP -> ThingThatActuallyDoesHTTP`
    and it juuust gets worse from there.
